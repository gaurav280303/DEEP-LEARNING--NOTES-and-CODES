{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VABzHTFzfD05"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences= [\n",
        "    \"The cat is sleeping.\",\n",
        "    \"I am going to school.\",\n",
        "    \"She loves playing football.\",\n",
        "    \"The dog is very happy.\",\n",
        "    \"They are eating breakfast.\",\n",
        "    \"He will buy a new phone.\",\n",
        "    \"The sun rises in the east.\",\n",
        "    \"I love reading books.\",\n",
        "    \"She is learning Python.\",\n",
        "    \"They are playing cricket.\",\n",
        "    \"The baby is laughing.\",\n",
        "    \"I will eat an apple.\",\n",
        "    \"He is watching TV.\",\n",
        "    \"She loves her family.\",\n",
        "    \"The flowers are blooming.\",\n",
        "    \"They are going to cinema.\",\n",
        "    \"I am feeling tired.\",\n",
        "    \"He will join a gym.\",\n",
        "    \"The book is very old.\",\n",
        "    \"She is a great singer.\",\n",
        "    \"I love Indian food.\",\n",
        "    \"They are best friends.\",\n",
        "    \"The dog is barking.\",\n",
        "    \"He is very smart.\",\n",
        "    \"The baby is crying.\",\n",
        "    \"I am learning Hindi.\",\n",
        "    \"She is very kind.\",\n",
        "    \"They are going home.\",\n",
        "    \"The sun sets in west.\",\n",
        "    \"I love my mom.\",\n",
        "    \"He is playing guitar.\",\n",
        "    \"She is a doctor.\",\n",
        "    \"The cat is eating.\",\n",
        "    \"They are studying hard.\",\n",
        "    \"I will travel soon.\",\n",
        "    \"He is very tall.\",\n",
        "    \"The flowers are red.\",\n",
        "    \"She loves dancing.\",\n",
        "    \"They are playing chess.\",\n",
        "    \"I am feeling happy.\",\n",
        "    \"He is a bad boy.\",\n",
        "    \"The dog is not running.\",\n",
        "    \"She is not learning Java.\",\n",
        "    \"I love my family.\",\n",
        "    \"They are watching movie.\",\n",
        "    \"He is a criminal.\",\n",
        "    \"The baby is sleeping.\",\n",
        "    \"I am going to hell.\",\n",
        "    \"She loves fucking.\",\n",
        "    \"They are eating non-veg.\"\n",
        "]"
      ],
      "metadata": {
        "id": "JxMEv-SxgPcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels=[1]*25 + [0]*25 #first 25 sentences are positive[1], last 25 sentences are negative[0]\n",
        "labels=np.array(labels)"
      ],
      "metadata": {
        "id": "FuYLJ58uXpJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size=2000 #For a safer side i took 2000 (a big number)\n",
        "tokenizer=Tokenizer(num_words= vocab_size, oov_token= \"<OOV>\") ##oov = out of vocabulary\n",
        "tokenizer.fit_on_texts(sentences) #words ke numeric variables aa jaenge\n",
        "sequence=tokenizer.texts_to_sequences(sentences)\n",
        "maxlen= max(len(s) for s in sequence) #finding max length for padding purpose\n",
        "\n",
        "X=pad_sequences(sequence, maxlen= maxlen, padding=\"post\") #post means jab words complete ho jaaye tb padding krna , baad me zeroes lgana\n",
        "y=labels"
      ],
      "metadata": {
        "id": "yP5ltUvMgzU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Creating the Labels (The Answer Sheet)\n",
        "python\n",
        "labels=[1]*25 + [0]*25\n",
        "Translation: \"Make 25 copies of '1', then 25 copies of '0'\"\n",
        "\n",
        "What it does:\n",
        "\n",
        "First 25 sentences → Positive (labeled as 1)\n",
        "\n",
        "Last 25 sentences → Negative (labeled as 0)\n",
        "\n",
        "Visual Example:\n",
        "\n",
        "text\n",
        "Sentence 1: \"I love this movie!\" → Label: 1 (Positive)\n",
        "Sentence 2: \"Amazing film!\" → Label: 1 (Positive)\n",
        "...\n",
        "Sentence 25: \"Great acting!\" → Label: 1 (Positive)\n",
        "Sentence 26: \"I hated it\" → Label: 0 (Negative)\n",
        "Sentence 27: \"Boring story\" → Label: 0 (Negative)\n",
        "...\n",
        "Sentence 50: \"Worst ever\" → Label: 0 (Negative)\n",
        "python\n",
        "labels=np.array(labels)\n",
        "Translation: \"Convert this Python list into a NumPy array\"\n",
        "Why: NumPy arrays are faster for mathematical operations and work better with machine learning libraries.\n",
        "\n",
        "2. Setting Up Vocabulary Size\n",
        "python\n",
        "vocab_size=2000\n",
        "Translation: \"Our dictionary will have space for 2000 words\"\n",
        "\n",
        "What it means:\n",
        "\n",
        "We're saying: \"Only keep track of the 2000 most common words\"\n",
        "\n",
        "If there are more than 2000 unique words in our sentences, the least common ones will be ignored\n",
        "\n",
        "2000 is a safe big number to make sure we don't miss important words\n",
        "\n",
        "Analogy: Imagine you have a notebook with 2000 pages. Each page is for a different word. If you encounter more than 2000 words, you don't get more pages!\n",
        "\n",
        "3. Creating the Tokenizer (The Word→Number Translator)\n",
        "python\n",
        "tokenizer=Tokenizer(num_words= vocab_size, oov_token= \"<OOV>\")\n",
        "Let's break this down:\n",
        "\n",
        "Part A: Tokenizer(num_words=vocab_size)\n",
        "\n",
        "Creates a \"translator\" that converts words to numbers\n",
        "\n",
        "num_words=2000 → Only remember top 2000 words\n",
        "\n",
        "Part B: oov_token=\"<OOV>\" (Most Important!)\n",
        "\n",
        "OOV = Out Of Vocabulary\n",
        "\n",
        "This creates a special code for unknown words\n",
        "\n",
        "Example: If \"supercalifragilisticexpialidocious\" isn't in our 2000 words, it becomes \"<OOV>\"\n",
        "\n",
        "Why OOV is crucial:\n",
        "Without OOV: Unknown word → Error/ignore\n",
        "With OOV: Unknown word → \"<OOV>\" → Gets a number → Can be processed!\n",
        "\n",
        "Analogy: You're learning Spanish with a 2000-word dictionary. When you hear a word not in your dictionary, you write \"UNKNOWN\" instead of panicking!\n",
        "\n",
        "4. Learning the Vocabulary\n",
        "python\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "Translation: \"Look at all sentences and learn which words are most common\"\n",
        "\n",
        "What happens inside:\n",
        "\n",
        "The tokenizer reads ALL sentences\n",
        "\n",
        "Counts how many times each word appears\n",
        "\n",
        "Ranks words from most frequent to least frequent\n",
        "\n",
        "Assigns numbers:\n",
        "\n",
        "Most common word → 1\n",
        "\n",
        "Second most common → 2\n",
        "\n",
        "... up to 2000\n",
        "\n",
        "Example:\n",
        "If our sentences are movie reviews:\n",
        "\n",
        "\"movie\" appears 50 times → Might get number 1\n",
        "\n",
        "\"good\" appears 45 times → Might get number 2\n",
        "\n",
        "\"zebra\" appears 1 time → Might not be in top 2000 → Becomes \"<OOV>\"\n",
        "\n",
        "5. Converting Sentences to Numbers\n",
        "python\n",
        "sequence=tokenizer.texts_to_sequences(sentences)\n",
        "Translation: \"Now translate each sentence from words to numbers using our dictionary\"\n",
        "\n",
        "Example:\n",
        "Original sentence: \"I love this movie\"\n",
        "After tokenization: [4, 15, 27, 1] (假设数字)\n",
        "\n",
        "Detailed process:\n",
        "\n",
        "text\n",
        "Input: [\"I love this movie\", \"It was terrible\"]\n",
        "Step 1: Split into words: [[\"I\", \"love\", \"this\", \"movie\"], [\"It\", \"was\", \"terrible\"]]\n",
        "Step 2: Look up each word in dictionary:\n",
        "        \"I\" → 4\n",
        "        \"love\" → 15\n",
        "        \"this\" → 27\n",
        "        \"movie\" → 1\n",
        "        \"It\" → 8\n",
        "        \"was\" → 12\n",
        "        \"terrible\" → 32\n",
        "Step 3: Output: [[4, 15, 27, 1], [8, 12, 32]]\n",
        "6. Finding Maximum Sentence Length\n",
        "python\n",
        "maxlen= max(len(s) for s in sequence)\n",
        "Translation: \"Find the sentence with the most words\"\n",
        "\n",
        "What it does:\n",
        "\n",
        "Looks at all converted sentences (now as number sequences)\n",
        "\n",
        "Counts how many numbers are in each\n",
        "\n",
        "Finds the maximum count\n",
        "\n",
        "Example:\n",
        "\n",
        "text\n",
        "Sentence 1: [4, 15, 27, 1] → Length: 4 words\n",
        "Sentence 2: [8, 12, 32] → Length: 3 words\n",
        "Sentence 3: [2, 7, 9, 11, 13, 20] → Length: 6 words\n",
        "maxlen = 6 (from Sentence 3)\n",
        "Why we need this: Neural networks need ALL inputs to be the SAME length!\n",
        "\n",
        "7. Padding Sequences (Making All Sentences Equal Length)\n",
        "python\n",
        "X=pad_sequences(sequence, maxlen= maxlen, padding=\"post\")\n",
        "This is CRITICAL! Let's break it down:\n",
        "\n",
        "The Problem:\n",
        "Sentences have different lengths:\n",
        "\n",
        "\"Hi\" → 1 word\n",
        "\n",
        "\"I love you\" → 3 words\n",
        "\n",
        "\"The quick brown fox jumps over the lazy dog\" → 9 words\n",
        "\n",
        "Neural networks need fixed input size!\n",
        "\n",
        "The Solution: Padding\n",
        "Add zeros to make all sequences the same length.\n",
        "\n",
        "padding=\"post\" means: Add zeros AT THE END\n",
        "\n",
        "Example:\n",
        "\n",
        "text\n",
        "Original sequences (unequal lengths):\n",
        "[4, 15, 27, 1]        # Length 4\n",
        "[8, 12, 32]           # Length 3  \n",
        "[2, 7, 9, 11, 13, 20] # Length 6\n",
        "\n",
        "After padding (maxlen=6):\n",
        "[4, 15, 27, 1, 0, 0]   # Added 2 zeros at END\n",
        "[8, 12, 32, 0, 0, 0]   # Added 3 zeros at END  \n",
        "[2, 7, 9, 11, 13, 20] # No padding needed (already length 6)\n",
        "Alternative: padding=\"pre\"\n",
        "Would add zeros at the BEGINNING:\n",
        "\n",
        "text\n",
        "[0, 0, 4, 15, 27, 1]\n",
        "[0, 0, 0, 8, 12, 32]\n",
        "[2, 7, 9, 11, 13, 20]\n",
        "Why \"post\" padding is usually better:\n",
        "For sentences, the beginning is more important. We don't want to push important words to the middle by adding zeros at the start!\n",
        "\n",
        "8. Final Variables\n",
        "python\n",
        "y=labels\n",
        "Translation: \"Our answers/labels are called 'y'"
      ],
      "metadata": {
        "id": "F_wOfRW6kvL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2Ax5SyeH5nt",
        "outputId": "b7c784cf-17bc-492a-9af9-0f157e17ffda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X[0] #X[0] has 4 words , but as max len in sentences is 6 . it will create two zeroes to complete the array of 6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSGLcDoZIYsK",
        "outputId": "9e81e7a8-35fb-4643-e140-e303577b1add"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 3, 22,  2, 23,  0,  0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X[47] #5 words , it will crete 1 zero  to complete the array"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9rKABgzIdKY",
        "outputId": "691422cb-b867-4846-cacb-ef282062c36e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 4, 10, 12, 17, 89,  0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepaing the model"
      ],
      "metadata": {
        "id": "IysCkr1KJVXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Now here you have to specify the number of dimensions in the hidden layer\n",
        "\n",
        "embed_dim= 16 #Creating embeddings of words 16 dimensions\n",
        "rnn_units= 8 #8 neurons in hidden layer"
      ],
      "metadata": {
        "id": "69ZjwlXKJEM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "embed_dim = 16\n",
        "What This Means:\n",
        "\"Each word will be represented by 16 personality traits\"\n",
        "\n",
        "The Problem with Simple Numbers:\n",
        "Right now, \"movie\" = 3 and \"film\" = 4. The computer thinks:\n",
        "\n",
        "3 and 4 are just numbers\n",
        "\n",
        "3 is close to 4, far from 100\n",
        "\n",
        "But \"movie\" and \"film\" mean almost the SAME thing!\n",
        "\n",
        "\"movie\" (3) and \"zebra\" (100) are far apart numerically, but in meaning... also far apart!\n",
        "\n",
        "Current (Bad) Representation:\n",
        "\n",
        "text\n",
        "\"movie\" = [3]\n",
        "\"film\"  = [4]\n",
        "\"zebra\" = [100]\n",
        "Computer thinks: \"3 and 4 are similar, 100 is different\" ✓\n",
        "But for wrong reasons!\n",
        "\n",
        "The Solution: Word Embeddings\n",
        "Instead of one number, give each word 16 numbers (16 dimensions):\n",
        "\n",
        "After Embedding:\n",
        "\n",
        "text\n",
        "\"movie\" = [0.8, -0.2, 0.4, 0.1, -0.9, 0.3, 0.5, -0.1, 0.7, 0.2, -0.3, 0.6, 0.4, -0.5, 0.1, 0.9]\n",
        "\"film\"  = [0.7, -0.1, 0.5, 0.2, -0.8, 0.4, 0.6, -0.2, 0.6, 0.3, -0.4, 0.7, 0.5, -0.4, 0.2, 0.8]\n",
        "\"zebra\" = [-0.1, 0.9, -0.8, 0.3, 0.4, -0.7, 0.2, 0.8, -0.3, 0.6, 0.5, -0.2, -0.9, 0.7, 0.4, -0.5]\n",
        "What Each Dimension Represents (Conceptually):\n",
        "Think of 16 personality tests for words:\n",
        "\n",
        "Dimension\tMight Represent\tExample Values\n",
        "1\tPositive/Negative\t\"good\"=0.9, \"bad\"=-0.8\n",
        "2\tObject/Action\t\"run\"=0.7, \"chair\"=-0.6\n",
        "3\tHuman/Thing\t\"teacher\"=0.8, \"rock\"=-0.9\n",
        "4\tSize\t\"elephant\"=0.9, \"ant\"=-0.9\n",
        "5\tSpeed\t\"rocket\"=0.8, \"snail\"=-0.7\n",
        "6\tAge\t\"ancient\"=0.9, \"new\"=-0.8\n",
        "7\tFormality\t\"thou\"=0.7, \"dude\"=-0.6\n",
        "...\t...\t...\n",
        "16\tEmotional Impact\t\"love\"=0.9, \"hate\"=-0.9\n",
        "Why 16 Dimensions?\n",
        "Too few (e.g., 2): Can't capture enough meaning\n",
        "\n",
        "text\n",
        "\"movie\" = [0.5, 0.5]\n",
        "\"film\" = [0.5, 0.5]\n",
        "\"zebra\" = [0.5, 0.5]  ← All look the same!\n",
        "Too many (e.g., 1000): Overcomplicated, slow to train\n",
        "\n",
        "16-300 is common: Balances richness with efficiency\n",
        "\n",
        "Visual Analogy:\n",
        "Imagine describing your friends:\n",
        "\n",
        "One number: \"Rate them 1-10\" → Limited!\n",
        "\n",
        "16 traits: [Funny: 8/10, Smart: 9/10, Kind: 7/10, Athletic: 6/10...] → Rich description!\n",
        "\n",
        "The network LEARNS these embeddings during training! It figures out what dimensions are useful.\n",
        "\n",
        "2. RNN Units: The Robot's BRAIN CELLS\n",
        "python\n",
        "rnn_units = 8\n",
        "What This Means:\n",
        "\"Our RNN robot has 8 thinking cells in its memory\"\n",
        "\n",
        "Understanding RNN Units:\n",
        "Each RNN unit is like a specialist in the robot's brain:\n",
        "\n",
        "text\n",
        "Think of 8 little workers in the robot's head:\n",
        "\n",
        "Worker 1: Specializes in \"Is this positive or negative?\"\n",
        "Worker 2: Specializes in \"Is this about people or things?\"\n",
        "Worker 3: Specializes in \"Is this past, present, or future?\"\n",
        "Worker 4: Specializes in \"Does this continue the sentence?\"\n",
        "Worker 5: Specializes in \"What's the main subject?\"\n",
        "Worker 6: Specializes in \"What's the action?\"\n",
        "Worker 7: Specializes in \"How intense is the emotion?\"\n",
        "Worker 8: Specializes in \"Is this a complete thought?\""
      ],
      "metadata": {
        "id": "ZR9S1E2-pVMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Embedding, SimpleRNN, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "inp=Input(shape=(maxlen,), dtype=\"int32\", name='input') #comma(,) for  if in case there is 2d vector      #5 inputs\n",
        "x=Embedding(input_dim=vocab_size, output_dim=embed_dim, mask_zero=True, name='embed')(inp)"
      ],
      "metadata": {
        "id": "qAfbpmITJqwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "inp = Input(shape=(maxlen,), dtype=\"int32\", name='input')\n",
        "Breaking it down piece by piece:\n",
        "A. Input()\n",
        "\n",
        "Analogy: Building a reception desk where data will arrive\n",
        "\n",
        "Purpose: Creates a placeholder where our sentences will enter the network\n",
        "\n",
        "This is NOT a processing layer - just an entry point\n",
        "\n",
        "B. shape=(maxlen,)\n",
        "This is CRITICAL! Let's understand:\n",
        "\n",
        "maxlen = Maximum sentence length we calculated earlier\n",
        "\n",
        "Example: If longest sentence has 20 words, maxlen=20\n",
        "\n",
        "Why the comma? (maxlen,) vs (maxlen)\n",
        "\n",
        "text\n",
        "(maxlen)   → Just a number, not a tuple\n",
        "(maxlen,)  → A 1-dimensional tuple (vector)\n",
        "(maxlen,5) → A 2-dimensional tuple (matrix)\n",
        "Example:\n",
        "\n",
        "Sentence with 20 words: [4, 15, 27, 1, 0, 0, ... 20 numbers total]\n",
        "\n",
        "Shape = (20,) ← 1D vector of length 20\n",
        "\n",
        "Not (20,1) (that would be 20 rows, 1 column)\n",
        "\n",
        "C. dtype=\"int32\"\n",
        "\n",
        "Data type: 32-bit integers\n",
        "\n",
        "Why integers? Because our words are encoded as numbers:\n",
        "\n",
        "text\n",
        "\"I\" = 4, \"love\" = 15, \"movie\" = 1\n",
        "Why not strings? Neural networks only understand numbers!\n",
        "\n",
        "D. name='input'\n",
        "\n",
        "Just a label for organization\n",
        "\n",
        "Helpful when debugging: \"Error at layer 'input'...\"\n",
        "\n",
        "What this creates:\n",
        "Imagine a mailbox slot shaped exactly for our sentences:\n",
        "\n",
        "text\n",
        "[INPUT LAYER]\n",
        "Shape: (20,)  ← Can accept 20 numbers at a time\n",
        "Type: Integers (word indices)\n",
        "Name: \"input\" (just a label)\n",
        "Visual:\n",
        "\n",
        "text\n",
        "Sentence: \"I love this movie\"\n",
        "Tokenized: [4, 15, 27, 1, 0, 0, 0, 0, ... up to 20 numbers]\n",
        "          ↓\n",
        "     [INPUT LAYER]\n",
        "     Shape: (20,)\n",
        "     Accepts: [4, 15, 27, 1, 0, 0, ...]\n",
        "2. The Embedding Layer: The Word \"Personality\" Factory\n",
        "python\n",
        "x = Embedding(input_dim=vocab_size, output_dim=embed_dim, mask_zero=True, name='embed')(inp)\n",
        "This is where the MAGIC HAPPENS! Let's break down each parameter:\n",
        "\n",
        "A. Embedding() - What is this?\n",
        "Analogy: A translation office that converts word IDs into rich descriptions\n",
        "\n",
        "Input: Word indices (like 4, 15, 27)\n",
        "\n",
        "Output: 16-dimensional vectors (personality profiles)\n",
        "\n",
        "B. Parameters Explained:\n",
        "1. input_dim=vocab_size\n",
        "\n",
        "vocab_size = 2000 (number of words in our dictionary)\n",
        "\n",
        "This creates a lookup table with 2000 entries\n",
        "\n",
        "Analogy: A dictionary with 2000 pages\n",
        "\n",
        "text\n",
        "Embedding Matrix (Conceptual):\n",
        "Row 0: [0.0, 0.0, 0.0, ... 16 zeros]  ← Usually reserved for padding\n",
        "Row 1: [0.3, -0.1, 0.8, ...]          ← Word ID 1 (most common word)\n",
        "Row 2: [0.1, 0.5, -0.3, ...]          ← Word ID 2\n",
        "...\n",
        "Row 1999: [0.7, 0.2, 0.4, ...]        ← Word ID 1999\n",
        "2. output_dim=embed_dim\n",
        "\n",
        "embed_dim = 16 (we set this earlier)\n",
        "\n",
        "Each word becomes a 16-number vector\n",
        "\n",
        "Analogy: Each dictionary entry has 16 personality traits\n",
        "\n",
        "**3. mask_zero=True ← THIS IS SUPER IMPORTANT!\n",
        "\n",
        "The Problem:\n",
        "Remember our padded sentences?\n",
        "\n",
        "text\n",
        "\"I love this movie\" → [4, 15, 27, 1, 0, 0, 0, 0, ...]\n",
        "Those zeros at the end are PADDING (not real words)!\n",
        "\n",
        "What mask_zero=True does:\n",
        "\n",
        "Tells the network: \"Ignore the zeros!\"\n",
        "\n",
        "Creates a mask that says: \"Process [4, 15, 27, 1], skip [0, 0, 0, 0]\"\n",
        "\n",
        "Without this: Network tries to learn from padding zeros (wastes time!)\n",
        "\n",
        "Analogy: Reading a form with blank spaces\n",
        "\n",
        "Without mask: \"Hmm, this blank space means something...\"\n",
        "\n",
        "With mask: \"Ah, blank space = ignore, focus on filled parts!\"\n",
        "\n",
        "4. name='embed'\n",
        "\n",
        "Just a label: \"This is the embedding layer\"\n",
        "\n",
        "5. (inp) at the end\n",
        "\n",
        "This connects the layers!\n",
        "\n",
        "Read it as: \"Take the output from inp layer, feed it into Embedding layer\"\n",
        "\n",
        "In Keras: Layer()(previous_layer_output)\n",
        "\n",
        "What happens INSIDE the Embedding Layer:\n",
        "Step 1: Input arrives\n",
        "\n",
        "text\n",
        "Sentence: [4, 15, 27, 1, 0, 0, 0, 0]\n",
        "Step 2: Lookup each number in embedding matrix\n",
        "\n",
        "text\n",
        "4 → Row 4 in embedding matrix → [0.1, 0.3, -0.2, ... 16 numbers]\n",
        "15 → Row 15 → [0.4, -0.1, 0.7, ...]\n",
        "27 → Row 27 → [0.2, 0.5, 0.1, ...]\n",
        "1 → Row 1 → [0.3, -0.2, 0.4, ...]\n",
        "0 → Row 0 → [0.0, 0.0, 0.0, ...] ← BUT masked! (ignored!)\n",
        "Step 3: Output shape transformation\n",
        "\n",
        "text\n",
        "Input shape: (batch_size, 20)  ← 20 word indices\n",
        "Output shape: (batch_size, 20, 16) ← Each word → 16D vector\n",
        "3. Complete Flow Example\n",
        "Let's trace one sentence through:\n",
        "\n",
        "Sentence: \"I love dogs\" (maxlen=20)\n",
        "Step 1: Tokenization & Padding\n",
        "\n",
        "text\n",
        "Original: \"I love dogs\"\n",
        "Tokenized: [4, 15, 32]  (假设数字)\n",
        "Padded: [4, 15, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "Step 2: Input Layer\n",
        "\n",
        "text\n",
        "Input layer receives: [4, 15, 32, 0, 0, 0, ...]\n",
        "Shape: (20,)  ← 20 numbers\n",
        "Step 3: Embedding Layer\n",
        "For EACH of the 20 positions:\n",
        "\n",
        "text\n",
        "Position 0: 4 → Lookup row 4 → [16 numbers]\n",
        "Position 1: 15 → Lookup row 15 → [16 numbers]\n",
        "Position 2: 32 → Lookup row 32 → [16 numbers]\n",
        "Positions 3-19: 0 → [16 zeros] BUT MASKED! (ignored)\n",
        "Final output from embedding layer:\n",
        "\n",
        "text\n",
        "Shape: (20, 16)  ← 20 positions, each with 16D vector\n",
        "\n",
        "Actual values:\n",
        "Position 0: [0.1, 0.3, -0.2, 0.4, ... 16 numbers]  ← \"I\"\n",
        "Position 1: [0.4, -0.1, 0.7, 0.2, ... 16 numbers]  ← \"love\"\n",
        "Position 2: [0.6, 0.2, -0.3, 0.5, ... 16 numbers]  ← \"dogs\"\n",
        "Positions 3-19: [0.0, 0.0, ...] but MARKED TO BE IGNORED\n",
        "4. The Embedding Matrix: A Closer Look\n",
        "The Embedding layer creates a trainable matrix:\n",
        "\n",
        "text\n",
        "Embedding Matrix (vocab_size × embed_dim)\n",
        "Rows: 2000 (one per word)\n",
        "Columns: 16 (dimensions)\n",
        "\n",
        "      Dimension1 Dimension2 ... Dimension16\n",
        "Word1   0.3        -0.1    ...    0.8\n",
        "Word2   0.1        0.5     ...   -0.3\n",
        "Word3  -0.2        0.7     ...    0.4\n",
        "...     ...        ...     ...    ...\n",
        "Word2000 0.7       0.2     ...    0.1\n",
        "Initially: Random numbers\n",
        "During training: Adjusted to make similar words have similar vectors\n",
        "\n",
        "5. Why This Architecture?\n",
        "Without Embedding Layer:\n",
        "text\n",
        "Input: [4, 15, 32] (just numbers)\n",
        "Problem: 4 and 15 are \"close\" numerically but \"I\" and \"love\" aren't that similar!\n",
        "With Embedding Layer:\n",
        "text\n",
        "Input: [4, 15, 32]\n",
        "→ Lookup in embedding matrix\n",
        "→ Output: Rich 16D vectors that CAPTURE MEANING\n",
        "→ Similar words have similar vectors\n",
        "6. The Masking Magic\n",
        "What mask_zero=True actually creates:\n",
        "\n",
        "text\n",
        "Input sentence: [4, 15, 32, 0, 0, 0, ...]\n",
        "               ↓\n",
        "Mask created:  [1, 1, 1, 0, 0, 0, ...]\n",
        "                ↑   ↑   ↑  ↑  ↑  ↑\n",
        "              Real Real RealPad Pad Pad\n",
        "              word word word\n",
        "This mask gets passed to subsequent layers (RNN, Dense, etc.), telling them:\n",
        "\n",
        "\"Pay attention to positions with mask=1\"\n",
        "\n",
        "\"Ignore positions with mask=0\"\n",
        "\n",
        "Without masking: RNN would process all 20 positions (wasting computation)\n",
        "With masking: RNN processes only 3 real words!"
      ],
      "metadata": {
        "id": "IjY75FZstgGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnn=SimpleRNN(units=rnn_units, return_sequences=True, return_state=True, name='simple_rnn')\n",
        "rnn_outputs, final_state = rnn(x) # Unpack the outputs: rnn_outputs is the sequence, final_state is the last hidden state\n",
        "output_layer=Dense(1, activation='sigmoid', name='output')(final_state) # Pass only the final_state to the Dense layer\n",
        "model=Model(inputs=inp, outputs=output_layer)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "_Lsp8fRattpp",
        "outputId": "516bf877-2bc0-4bf7-9d9e-8b0b292da114"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embed (\u001b[38;5;33mEmbedding\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │     \u001b[38;5;34m32,000\u001b[0m │ input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ simple_rnn          │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m8\u001b[0m),    │        \u001b[38;5;34m200\u001b[0m │ embed[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
              "│ (\u001b[38;5;33mSimpleRNN\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)]        │            │ not_equal_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m9\u001b[0m │ simple_rnn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m]  │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embed (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,000</span> │ input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ simple_rnn          │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>),    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span> │ embed[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)]        │            │ not_equal_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span> │ simple_rnn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]  │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m32,209\u001b[0m (125.82 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">32,209</span> (125.82 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m32,209\u001b[0m (125.82 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">32,209</span> (125.82 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Creating the RNN Layer: The Brain with Memory\n",
        "python\n",
        "rnn = SimpleRNN(units=rnn_units, return_sequences=True, return_state=True, name='simple_rnn')\n",
        "Breaking down each parameter:\n",
        "A. SimpleRNN()\n",
        "\n",
        "This is the simplest type of RNN\n",
        "\n",
        "Analogy: A basic brain cell that remembers things temporarily\n",
        "\n",
        "B. units=rnn_units\n",
        "\n",
        "rnn_units = 8 (we set this earlier)\n",
        "\n",
        "This means: 8 memory cells in our RNN\n",
        "\n",
        "Each cell produces ONE number at each time step\n",
        "\n",
        "C. return_sequences=True ← IMPORTANT!\n",
        "\n",
        "What does this mean?\n",
        "\n",
        "\"Give me the output at EVERY time step\"\n",
        "\n",
        "Not just the final output, but outputs at ALL positions\n",
        "\n",
        "Example with sentence \"I love dogs\":\n",
        "\n",
        "text\n",
        "Without return_sequences=True: Returns only last output\n",
        "[I] → [love] → [dogs] → [final_output]\n",
        "\n",
        "With return_sequences=True: Returns outputs at each step\n",
        "[I] → output₁\n",
        "[love] → output₂  \n",
        "[dogs] → output₃\n",
        "Returns: [output₁, output₂, output₃]\n",
        "Why do we need this? For visualization/debugging! Though in our case, we only use the final state.\n",
        "\n",
        "D. return_state=True\n",
        "\n",
        "\"Also give me the final memory state\"\n",
        "\n",
        "The state is the last hidden state (memory after processing entire sequence)\n",
        "\n",
        "E. name='simple_rnn'\n",
        "\n",
        "Just a label for organization\n",
        "\n",
        "2. Running the RNN: Processing Our Sentence\n",
        "python\n",
        "rnn_outputs, final_state = rnn(x)  # Unpack the outputs\n",
        "What happens here:\n",
        "Input x: From embedding layer, shape: (batch_size, maxlen, embed_dim)\n",
        "Example: For one sentence with maxlen=20, embed_dim=16:\n",
        "\n",
        "Shape: (1, 20, 16) ← 1 sentence, 20 positions, each position 16D vector\n",
        "\n",
        "The RNN processes EACH position:\n",
        "\n",
        "text\n",
        "Time step 0: Process word 0 (\"I\" = 16D vector) → Update memory → output₀\n",
        "Time step 1: Process word 1 (\"love\" = 16D vector) + previous memory → Update memory → output₁  \n",
        "Time step 2: Process word 2 (\"dogs\" = 16D vector) + previous memory → Update memory → output₂\n",
        "Time steps 3-19: Process padding (0 vectors, but MASKED! So actually skipped!)\n",
        "The TWO outputs we get:\n",
        "1. rnn_outputs: Outputs at EVERY time step (because return_sequences=True)\n",
        "\n",
        "Shape: (batch_size, maxlen, rnn_units)\n",
        "\n",
        "Example: (1, 20, 8) ← For each of 20 positions, 8 numbers (from 8 RNN units)\n",
        "\n",
        "2. final_state: Final memory after processing ALL words\n",
        "\n",
        "Shape: (batch_size, rnn_units)\n",
        "\n",
        "Example: (1, 8) ← Just 8 numbers representing final memory\n",
        "\n",
        "Visual Example:\n",
        "\n",
        "text\n",
        "Input: \"I love dogs\" (padded to 20 words)\n",
        "After RNN processing:\n",
        "\n",
        "rnn_outputs = [\n",
        "    [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8],  ← After \"I\"\n",
        "    [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],  ← After \"I love\"\n",
        "    [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],  ← After \"I love dogs\"\n",
        "    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],  ← Padding (masked)\n",
        "    ... 17 more zero vectors\n",
        "]\n",
        "\n",
        "final_state = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]  ← Last real output\n",
        "Important: Even though we get rnn_outputs (all sequences), we only use final_state for classification!\n",
        "\n",
        "3. The Dense Output Layer: Making the Decision\n",
        "python\n",
        "output_layer = Dense(1, activation='sigmoid', name='output')(final_state)\n",
        "Breaking this down:\n",
        "A. Dense()\n",
        "\n",
        "A fully connected layer (like in regular neural networks)\n",
        "\n",
        "Analogy: The \"decision-making committee\"\n",
        "\n",
        "B. units=1\n",
        "\n",
        "Only ONE neuron in this layer\n",
        "\n",
        "Why? Because we're doing binary classification (positive=1, negative=0)\n",
        "\n",
        "Single neuron outputs a probability between 0 and 1\n",
        "\n",
        "C. activation='sigmoid'\n",
        "\n",
        "Sigmoid function: Squishes ANY number to between 0 and 1\n",
        "\n",
        "Formula: sigmoid(x) = 1 / (1 + e^(-x))\n",
        "\n",
        "Output interpretation:\n",
        "\n",
        "Close to 1 → \"Probably positive sentiment\"\n",
        "\n",
        "Close to 0 → \"Probably negative sentiment\"\n",
        "\n",
        "0.5 → \"Unsure\"\n",
        "\n",
        "D. (final_state)\n",
        "\n",
        "We're feeding ONLY the final_state (not all rnn_outputs)\n",
        "\n",
        "Why? For sentiment analysis, we only care about the overall sentiment after reading the ENTIRE sentence\n",
        "\n",
        "The final_state contains the \"summary\" of the whole sentence\n",
        "\n",
        "What happens inside Dense layer:\n",
        "text\n",
        "final_state = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]  ← 8 numbers\n",
        "\n",
        "Dense layer does:\n",
        "output = sigmoid(W * final_state + b)\n",
        "\n",
        "Where:\n",
        "W = [w1, w2, w3, w4, w5, w6, w7, w8]  ← 8 weights (learned)\n",
        "b = bias (learned)\n",
        "\n",
        "Calculation:\n",
        "1. Multiply each memory number by corresponding weight\n",
        "2. Sum them all up\n",
        "3. Add bias\n",
        "4. Apply sigmoid to get probability\n",
        "\n",
        "Example: sigmoid(2.5) = 0.924  ← \"92.4% chance of positive sentiment\"\n",
        "4. Creating the Complete Model\n",
        "python\n",
        "model = Model(inputs=inp, outputs=output_layer)\n",
        "What this does:\n",
        "Connects everything together: Input → Embedding → RNN → Dense → Output\n",
        "\n",
        "Creates a trainable model that maps:\n",
        "\n",
        "text\n",
        "Sentences (word indices) → Sentiment probability\n",
        "Visual of the complete flow:\n",
        "\n",
        "text\n",
        "inp (Input layer)\n",
        "  ↓\n",
        "x (Embedding layer: words → 16D vectors)\n",
        "  ↓\n",
        "rnn_outputs, final_state (RNN layer: processes sequence)\n",
        "  ↓ (we only take final_state)\n",
        "final_state (8-number memory summary)\n",
        "  ↓\n",
        "output_layer (Dense: converts 8 numbers → 1 probability)\n",
        "5. Compiling the Model: Setting Up Training Rules\n",
        "python\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "A. optimizer='adam'\n",
        "Adam: An algorithm that adjusts weights during training\n",
        "\n",
        "Analogy: The \"learning strategy\" for our robot\n",
        "\n",
        "What it does: \"If you make a mistake, adjust weights this way to improve\"\n",
        "\n",
        "B. loss='binary_crossentropy'\n",
        "Loss function: Measures how wrong our predictions are\n",
        "\n",
        "Binary crossentropy: Perfect for yes/no (1/0) classification\n",
        "\n",
        "Formula (simplified): Penalizes confident wrong predictions more!\n",
        "\n",
        "Example:\n",
        "\n",
        "text\n",
        "Prediction: 0.9 (90% positive)\n",
        "Actual: 1 (positive) → Low loss (good!)\n",
        "Actual: 0 (negative) → High loss (confidently wrong!)\n",
        "\n",
        "Prediction: 0.6 (60% positive)  \n",
        "Actual: 0 (negative) → Medium loss (less confident, smaller penalty)\n",
        "C. metrics=['accuracy']\n",
        "Tells us what to measure during training\n",
        "\n",
        "Accuracy: Percentage of correct predictions\n",
        "\n",
        "Example: 85% accuracy = 85 out of 100 predictions correct\n",
        "\n",
        "6. Model Summary: Seeing Our Creation\n",
        "python\n",
        "model.summary()\n",
        "What this shows (example output):\n",
        "text\n",
        "Model: \"model\"\n",
        "_________________________________________________________________\n",
        "Layer (type)                 Output Shape              Param #   \n",
        "=================================================================\n",
        "input (InputLayer)           [(None, 20)]              0         \n",
        "_________________________________________________________________\n",
        "embed (Embedding)            (None, 20, 16)            32000     \n",
        "_________________________________________________________________\n",
        "simple_rnn (SimpleRNN)       [(None, 20, 8), (None, 8)] 200      \n",
        "_________________________________________________________________\n",
        "output (Dense)               (None, 1)                 9         \n",
        "=================================================================\n",
        "Total params: 32,209\n",
        "Trainable params: 32,209\n",
        "Non-trainable params: 0\n",
        "Let's understand each line:\n",
        "1. Input Layer:\n",
        "\n",
        "Output shape: (None, 20)\n",
        "\n",
        "None = Batch size (can vary)\n",
        "\n",
        "20 = maxlen (words per sentence)\n",
        "\n",
        "Parameters: 0 (just a placeholder)\n",
        "\n",
        "2. Embedding Layer:\n",
        "\n",
        "Output shape: (None, 20, 16)\n",
        "\n",
        "20 positions, each becomes 16-dimensional\n",
        "\n",
        "Parameters: 32,000!\n",
        "\n",
        "How? vocab_size × embed_dim = 2000 × 16 = 32,000\n",
        "\n",
        "Each of 2000 words has 16 numbers to learn\n",
        "\n",
        "3. RNN Layer:\n",
        "\n",
        "Output shape shows TWO things (because return_sequences=True, return_state=True):\n",
        "\n",
        "(None, 20, 8) = All sequence outputs\n",
        "\n",
        "(None, 8) = Final state only\n",
        "\n",
        "Parameters: 200\n",
        "\n",
        "How? Let's calculate:\n",
        "\n",
        "W_xh: Input to hidden: embed_dim × rnn_units = 16 × 8 = 128\n",
        "\n",
        "W_hh: Hidden to hidden: rnn_units × rnn_units = 8 × 8 = 64\n",
        "\n",
        "b_h: Bias: rnn_units = 8\n",
        "\n",
        "Total: 128 + 64 + 8 = 200\n",
        "\n",
        "4. Dense Layer:\n",
        "\n",
        "Output shape: (None, 1) ← Single probability\n",
        "\n",
        "Parameters: 9\n",
        "\n",
        "How? rnn_units × 1 + 1 = 8 × 1 + 1 = 9\n",
        "\n",
        "8 weights (one for each memory number) + 1 bias\n",
        "\n",
        "Total parameters: 32,209 weights/biases to learn!\n",
        "\n",
        "7. Complete Flow with Example Data\n",
        "Let's trace one sentence through the ENTIRE model:\n",
        "\n",
        "Sentence: \"I love this movie\"\n",
        "text\n",
        "Step 1: Input\n",
        "Sentence → Tokenized → Padded: [4, 15, 27, 1, 0, 0, ..., 0]  (20 numbers)\n",
        "\n",
        "Step 2: Embedding Layer\n",
        "Each number → 16D vector:\n",
        "4 → [0.1, 0.3, -0.2, ... 16 numbers]\n",
        "15 → [0.4, -0.1, 0.7, ...]\n",
        "27 → [0.2, 0.5, 0.1, ...]\n",
        "1 → [0.3, -0.2, 0.4, ...]\n",
        "0 → [0.0, 0.0, ...] (masked)\n",
        "...\n",
        "Shape becomes: (1, 20, 16)\n",
        "\n",
        "Step 3: RNN Layer (time step by time step):\n",
        "Time 0: Process \"I\" vector → Memory becomes [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "Time 1: Process \"love\" + previous memory → Memory: [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "Time 2: Process \"this\" + memory → Memory: [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
        "Time 3: Process \"movie\" + memory → Memory: [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1]\n",
        "Times 4-19: Padding (masked, so memory stays same)\n",
        "Final state = [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1]\n",
        "\n",
        "Step 4: Dense Layer\n",
        "Take final_state [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1]\n",
        "Multiply by learned weights, add bias, apply sigmoid:\n",
        "Example calculation: 0.4×0.1 + 0.5×0.2 + 0.6×0.3 + 0.7×0.4 + 0.8×0.5 + 0.9×0.6 + 1.0×0.7 + 1.1×0.8 + 0.1\n",
        "= 3.05 → sigmoid(3.05) = 0.955\n",
        "\n",
        "Step 5: Prediction\n",
        "Output: 0.955 ≈ 95.5% probability of positive sentiment ✓\n",
        "8. Why This Architecture for Sentiment Analysis?\n",
        "Alternative: Using ALL RNN outputs (not just final state)\n",
        "We could average ALL outputs or use the last one. But:\n",
        "\n",
        "Using final state: Captures cumulative understanding of ENTIRE sentence\n",
        "\n",
        "Averaging all outputs: Might dilute important words\n",
        "\n",
        "Alternative: More RNN layers\n",
        "We could stack multiple RNN layers:\n",
        "\n",
        "python\n",
        "rnn1 = SimpleRNN(units=8, return_sequences=True)(x)\n",
        "rnn2 = SimpleRNN(units=8)(rnn1)  # Second layer\n",
        "But for simple sentiment, one layer is often enough!\n",
        "\n",
        "Summary: The Complete Picture\n",
        "text\n",
        "1. Input: Sentences as word indices\n",
        "   [4, 15, 27, 1, 0, 0, ...]\n",
        "\n",
        "2. Embedding: Words → Rich 16D vectors\n",
        "   Each word gets 16 \"personality traits\"\n",
        "\n",
        "3. RNN: Process sequence, building memory\n",
        "   \"I\" → \"I love\" → \"I love this\" → \"I love this movie\"\n",
        "   Final state = Summary of entire sentence\n",
        "\n",
        "4. Dense: Convert 8D memory → Probability\n",
        "   8 numbers → 1 number (0 to 1)\n",
        "\n",
        "5. Output: Sentiment probability\n",
        "   0.95 = \"95% sure it's positive!\""
      ],
      "metadata": {
        "id": "WpMzNu_cyeQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X,y, epochs=25, batch_size=8, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRqsRx7ByhYk",
        "outputId": "c727c6f2-4cb2-4ff0-efb7-9cfa82c0cb4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.3720 - loss: 0.7104\n",
            "Epoch 2/25\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4754 - loss: 0.6908 \n",
            "Epoch 3/25\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6638 - loss: 0.6703 \n",
            "Epoch 4/25\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7566 - loss: 0.6520 \n",
            "Epoch 5/25\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7567 - loss: 0.6378 \n",
            "Epoch 6/25\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8969 - loss: 0.6157 \n",
            "Epoch 7/25\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8306 - loss: 0.6110 \n",
            "Epoch 8/25\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8509 - loss: 0.5801 \n",
            "Epoch 9/25\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8997 - loss: 0.5680 \n",
            "Epoch 10/25\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8634 - loss: 0.5462 \n",
            "Epoch 11/25\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8447 - loss: 0.5455 \n",
            "Epoch 12/25\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8671 - loss: 0.5103 \n",
            "Epoch 13/25\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9249 - loss: 0.4791 \n",
            "Epoch 14/25\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8826 - loss: 0.4904 \n",
            "Epoch 15/25\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9219 - loss: 0.4252 \n",
            "Epoch 16/25\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9418 - loss: 0.4077 \n",
            "Epoch 17/25\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9600 - loss: 0.3905 \n",
            "Epoch 18/25\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9676 - loss: 0.3573 \n",
            "Epoch 19/25\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9144 - loss: 0.3640 \n",
            "Epoch 20/25\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9525 - loss: 0.3255 \n",
            "Epoch 21/25\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9655 - loss: 0.3130 \n",
            "Epoch 22/25\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9525 - loss: 0.2872 \n",
            "Epoch 23/25\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9291 - loss: 0.3026 \n",
            "Epoch 24/25\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9893 - loss: 0.2639 \n",
            "Epoch 25/25\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9893 - loss: 0.2418 \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e97d3b80740>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ae55d40"
      },
      "source": [
        "# Task\n",
        "Create a Keras feature extraction model that takes the same input as the original model but outputs the `Embedding` layer's output (`x`) and the `SimpleRNN` layer's `rnn_outputs` (hidden states for each time step), and use it to extract and display these intermediate representations for the sample sentence \"The cat is sleeping.\". Include the shapes and a snippet of the numerical values for both the word embeddings and the RNN hidden states. Summarize the extracted embeddings and hidden states, explaining what these numerical representations signify in the context of the model's processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43cc9bd4"
      },
      "source": [
        "## Create Feature Extraction Model\n",
        "\n",
        "### Subtask:\n",
        "Build a new Keras model that takes the same input as the original model but outputs the `Embedding` layer's output (pre-timestamps) and the `SimpleRNN` layer's `rnn_outputs` (hidden states for each time step).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27850de2"
      },
      "source": [
        "**Reasoning**:\n",
        "To create the feature extraction model as requested, I will define a new Keras Model that uses the existing 'inp' as input and outputs both the 'x' (embedding layer output) and 'rnn_outputs' (RNN layer's sequence outputs).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "0b04a5bd",
        "outputId": "3d0eee6e-e8c2-4f6f-b06c-27e5e70c2a89"
      },
      "source": [
        "feature_extractor_model = Model(inputs=inp, outputs=[x, rnn_outputs])\n",
        "print(\"Feature extraction model created successfully.\")\n",
        "feature_extractor_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature extraction model created successfully.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embed (\u001b[38;5;33mEmbedding\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │     \u001b[38;5;34m32,000\u001b[0m │ input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ simple_rnn          │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m8\u001b[0m),    │        \u001b[38;5;34m200\u001b[0m │ embed[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
              "│ (\u001b[38;5;33mSimpleRNN\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)]        │            │ not_equal_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embed (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,000</span> │ input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ simple_rnn          │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>),    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span> │ embed[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)]        │            │ not_equal_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m32,200\u001b[0m (125.78 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">32,200</span> (125.78 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m32,200\u001b[0m (125.78 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">32,200</span> (125.78 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23d4723e"
      },
      "source": [
        "**Reasoning**:\n",
        "To extract the features, I need to prepare the sample sentence by tokenizing and padding it, then use the `feature_extractor_model` to predict the embedding layer output and the RNN hidden states.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36578221",
        "outputId": "c80163e7-ed98-42fd-9703-a8817ae5e084"
      },
      "source": [
        "sample_sentence = \"The cat is sleeping.\"\n",
        "sample_sequence = tokenizer.texts_to_sequences([sample_sentence])\n",
        "sample_padded = pad_sequences(sample_sequence, maxlen=maxlen, padding=\"post\")\n",
        "\n",
        "embeddings, rnn_hidden_states = feature_extractor_model.predict(sample_padded)\n",
        "\n",
        "print(f\"\\nWord Embeddings for '{sample_sentence}':\")\n",
        "print(f\"Shape: {embeddings.shape}\")\n",
        "print(\"Snippet (first 3 words, first 5 dimensions):\")\n",
        "print(embeddings[0, :3, :5])\n",
        "\n",
        "print(f\"\\nRNN Hidden States for '{sample_sentence}':\")\n",
        "print(f\"Shape: {rnn_hidden_states.shape}\")\n",
        "print(\"Snippet (first 3 time steps, first 5 dimensions):\")\n",
        "print(rnn_hidden_states[0, :3, :5])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step\n",
            "\n",
            "Word Embeddings for 'The cat is sleeping.':\n",
            "Shape: (1, 6, 16)\n",
            "Snippet (first 3 words, first 5 dimensions):\n",
            "[[ 0.02297346 -0.02816715 -0.01602104 -0.0534954   0.01294242]\n",
            " [-0.04681893 -0.04857499 -0.022462   -0.0228855  -0.02925644]\n",
            " [ 0.00024168 -0.02866628  0.00211756  0.00082364 -0.02142565]]\n",
            "\n",
            "RNN Hidden States for 'The cat is sleeping.':\n",
            "Shape: (1, 6, 8)\n",
            "Snippet (first 3 time steps, first 5 dimensions):\n",
            "[[-0.03633358  0.04461361 -0.05488749  0.09265917 -0.0275171 ]\n",
            " [-0.09269962 -0.04962511 -0.07743929 -0.04999477  0.01358555]\n",
            " [-0.01875091 -0.05721251 -0.07047111  0.09739455  0.10126595]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe807b5d"
      },
      "source": [
        "### Summary of Extracted Embeddings and Hidden States\n",
        "\n",
        "#### Word Embeddings (Output from `Embedding` layer: `embeddings`)\n",
        "\n",
        "**Shape:** (1, 6, 16)\n",
        "\n",
        "**Significance:**\n",
        "*   **Conceptual:** Each word in the sentence \"The cat is sleeping.\" has been transformed from a simple integer ID into a 16-dimensional vector of floating-point numbers. These numbers are learned during the model's training and represent the semantic meaning or \"personality traits\" of each word. Words with similar meanings tend to have similar embedding vectors (i.e., their vectors are close in the 16-dimensional space).\n",
        "*   **Example Snippet:**\n",
        "    ```\n",
        "    [[ 0.02297346 -0.02816715 -0.01602104 -0.0534954   0.01294242]\n",
        "     [-0.04681893 -0.04857499 -0.022462   -0.0228855  -0.02925644]\n",
        "     [ 0.00024168 -0.02866628  0.00211756  0.00082364 -0.02142565]]\n",
        "    ```\n",
        "    - The first row corresponds to the embedding of \"The\", the second to \"cat\", and the third to \"is\". Each number within a row contributes to defining that word's context and relationship to other words. The `0` values from padding words would also have corresponding 16-D vectors, but `mask_zero=True` ensures the RNN ignores them during processing.\n",
        "\n",
        "#### RNN Hidden States (Output from `SimpleRNN` layer: `rnn_hidden_states`)\n",
        "\n",
        "**Shape:** (1, 6, 8)\n",
        "\n",
        "**Significance:**\n",
        "*   **Conceptual:** The RNN hidden states represent the \"memory\" or contextual understanding of the sentence as the RNN processes it word by word. Since `return_sequences=True` was set, we get a hidden state vector (of 8 dimensions) for *each* time step (or word position) in the input sequence, including the padded zeros. Each 8-dimensional vector at a given time step encapsulates information about the current word *and* all preceding words in the sequence.\n",
        "*   **Example Snippet:**\n",
        "    ```\n",
        "    [[-0.03633358  0.04461361 -0.05488749  0.09265917 -0.0275171 ]\n",
        "     [-0.09269962 -0.04962511 -0.07743929 -0.04999477  0.01358555]\n",
        "     [-0.01875091 -0.05721251 -0.07047111  0.09739455  0.10126595]]\n",
        "    ```\n",
        "    - The first row is the hidden state after processing \"The\", the second after \"The cat\", and the third after \"The cat is\". These vectors are dynamic; they change and accumulate information as the RNN reads the sentence. The hidden states corresponding to padded zeros in the input would typically reflect the state after the last meaningful word, as the masking mechanism instructs the RNN to ignore padding and not update its state for them.\n",
        "\n",
        "In essence, the **word embeddings** provide a rich, static representation for each individual word, while the **RNN hidden states** provide a dynamic, context-aware representation of the sentence's meaning as it unfolds sequentially."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95138e3d"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "*   **What do the extracted word embeddings signify in the context of the model's processing?**\n",
        "    Word embeddings are 16-dimensional vectors that represent the semantic meaning of each word. These numbers are learned during training, and words with similar meanings tend to have similar embedding vectors, signifying their contextual and semantic relationships.\n",
        "\n",
        "*   **What do the extracted RNN hidden states signify in the context of the model's processing?**\n",
        "    RNN hidden states are 8-dimensional vectors that represent the \"memory\" or contextual understanding of the sentence as the RNN processes it word by word. Each hidden state at a given time step encapsulates information about the current word and all preceding words, providing a dynamic, context-aware representation of the sentence's meaning as it unfolds.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   A Keras feature extraction model was successfully created, which takes the same input as the original model and outputs both the `Embedding` layer's output and the `SimpleRNN` layer's sequence outputs.\n",
        "*   For the sample sentence \"The cat is sleeping.\", the model extracted:\n",
        "    *   **Word Embeddings**: These have a shape of `(1, 6, 16)`, indicating 1 input sequence, 6 time steps (words/padding), and 16 dimensions per embedding. A snippet showed initial values like `[[ 0.02297346 -0.02816715 -0.01602104 -0.0534954 0.01294242] ...]` for the first word.\n",
        "    *   **RNN Hidden States**: These have a shape of `(1, 6, 8)`, indicating 1 input sequence, 6 time steps, and 8 dimensions for each hidden state. A snippet showed initial values like `[[-0.03633358 0.04461361 -0.05488749 0.09265917 -0.0275171 ] ...]` for the first time step.\n",
        "*   Word embeddings provide a rich, static representation for individual words, while RNN hidden states offer a dynamic, context-aware representation of the sentence's meaning as it is processed sequentially.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   This feature extraction model provides a powerful tool for understanding how the neural network processes text, allowing for visualization and analysis of semantic word representations and contextual sentence understanding.\n",
        "*   Further analysis could involve plotting these embeddings and hidden states (e.g., using dimensionality reduction techniques like t-SNE) to visually inspect word relationships or how context evolves over a sentence, potentially revealing model biases or specific learning patterns.\n"
      ]
    }
  ]
}